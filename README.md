# transformer

由于论文使用的数据集过大，于是这里使用了比较小的英文译德文数据集Multi30K

按照论文中的所述基本超参设定，并没有使用论文中提及的 BPE 分词（其分词过时，其分词器耗时过高，跑起来很慢）。于是使用了最基本的，按照单词进行分词。
并未使用论文中所提到的学习率调度策略，因为其训练轮次过大（100K轮），经测试，其策略不适合该数据集。
除上述两点之外，自己实现的Transformer模型严格按照论文要求进行编写。

在实现中，将忽略出现次数过少的分词（设置为1、2、5对预测结果影响不大），代码中提供了基于论文数据集的对应optimizer、scheduler等。也提供了经过测试后适合数据集Multi30k对应的optimizer、scheduler，并加上了若干常见的防止过拟合的超参。

另外，对验证集验证部分，提供了使用教师机制（teacher forcing）以及不使用教师机制的验证策略供调用。对于翻译准确度计算使用了bleu。

不过由于使用的是最基本的Transformer模型，加上训练集小等原因，epoch上去后将出现过拟合。
